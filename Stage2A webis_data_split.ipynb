{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"w19MG5BUJR9K","outputId":"996c8b27-932f-4a4a-fdc6-4042a628f1c0"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\fahan\\Program files\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import nltk\n","from tqdm import tqdm\n","tqdm.pandas()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nVEctoyJR9N","outputId":"0af8ed66-7c23-4497-cb4e-6d18b432efc6"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\fahan\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSzfkXigJR9N","outputId":"78f54380-7856-4943-aa92-b64c1bc9ef59"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\fahan\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping corpora\\stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"60r5NhcqJR9O"},"outputs":[],"source":["def label_extractor(filename):\n","    with open(filename, 'r', encoding='utf-8', errors='ignore') as fh:\n","        for line in fh:\n","            if line.startswith('Paraphrase'):\n","                return(line.strip('\\n').split(':')[-1].strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFxUohrLJR9O"},"outputs":[],"source":["def process_document(file, one_sentence=True):\n","    # put text in all lower case letters \n","    all_text = file.read().lower()\n","    if(one_sentence):\n","        all_text = re.sub(r\"[^a-zA-Z0-9]\", \" \", all_text)\n","    else:\n","        all_text = re.sub(r\"[^a-zA-Z0-9\\.?!]\", \" \", all_text)\n","    # remove newlines/tabs, etc. so it's easier to match phrases, later\n","    all_text = re.sub(r\"\\t\", \" \", all_text)\n","    all_text = re.sub(r\"\\n\", \" \", all_text)\n","    all_text = re.sub(r\"\\s+\", \" \", all_text)\n","    return(all_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VnO379NqJR9O"},"outputs":[],"source":["def clean_text(text):\n","    cl_txt = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n","    cl_txt = re.sub(r\"\\s$\", \"\", cl_txt)\n","    return(cl_txt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZ9MPlucJR9P"},"outputs":[],"source":["def doc2sent(filename):\n","    with open(filename, 'r', encoding='utf-8', errors='ignore') as fh:\n","        processed_text = process_document(fh, one_sentence=False)\n","        sentences = nltk.sent_tokenize(processed_text)\n","        sentences = [clean_text(sent) for sent in sentences]\n","    return(sentences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPOnQYxxJR9P"},"outputs":[],"source":["def doc2onesent(filename):\n","    with open(filename, 'r', encoding='utf-8', errors='ignore') as fh:\n","        processed_text = process_document(fh)\n","    return(processed_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEa4zBhSJR9P","outputId":"27386ad3-1577-4066-c1d5-281112d72b67"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|█████████████████████████████████████████████████████████████████████████████| 7859/7859 [00:39<00:00, 197.02it/s]\n"]}],"source":["# directory = r'C:/Users/fahan/Desktop/CENTELLA/NLP/plagiarism_detection_project/Webis-CPC-11'\n","# num_files = len([filename for filename in os.listdir(directory)])//3\n","# original_filename = []\n","# paraphrase_filename = []\n","# label = []\n","# original_sent = []\n","# original_sent_count =[]\n","# paraphrase_sent = []\n","# paraphrase_sent_count = []\n","# for i in tqdm(range(1,num_files+1)):\n","#     original_filename.append(f'{i}-original.txt')\n","#     paraphrase_filename.append(f'{i}-paraphrase.txt')\n","#     label.append(label_extractor(directory+fr'\\{i}-metadata.txt'))\n","#     original_sent.append(doc2sent(directory+fr'\\{i}-original.txt'))\n","#     original_sent_count.append(len(original_sent[-1]))\n","#     paraphrase_sent.append(doc2sent(directory+fr'\\{i}-paraphrase.txt'))\n","#     paraphrase_sent_count.append(len(paraphrase_sent[-1]))\n","# webis_df_multi_sent = pd.DataFrame({'original_file':original_filename,'paraphrase_file':paraphrase_filename,\n","#                           'label':label,'original_text':original_sent,'paraphrase_text':paraphrase_sent,\n","#                          'original_sent_count':original_sent_count,'paraphrase_sent_count':paraphrase_sent_count})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"emNGypagJR9Q","outputId":"be47c07b-eee0-460c-a134-356c9b27731c"},"outputs":[{"data":{"text/plain":["(7859, 3)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>original_file</th>\n","      <th>paraphrase_file</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1-original.txt</td>\n","      <td>1-paraphrase.txt</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2-original.txt</td>\n","      <td>2-paraphrase.txt</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3-original.txt</td>\n","      <td>3-paraphrase.txt</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4-original.txt</td>\n","      <td>4-paraphrase.txt</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5-original.txt</td>\n","      <td>5-paraphrase.txt</td>\n","      <td>No</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    original_file   paraphrase_file label\n","0  1-original.txt  1-paraphrase.txt   Yes\n","1  2-original.txt  2-paraphrase.txt   Yes\n","2  3-original.txt  3-paraphrase.txt   Yes\n","3  4-original.txt  4-paraphrase.txt   Yes\n","4  5-original.txt  5-paraphrase.txt    No"]},"metadata":{},"output_type":"display_data"}],"source":["webis_df_initial = pd.DataFrame({'original_file':original_filename,'paraphrase_file':paraphrase_filename,\n","                          'label':label})\n","display(webis_df_initial.shape)\n","display(webis_df_initial.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_2cSgvfJR9Q"},"outputs":[],"source":["# webis_df_multi_sent = webis_df_multi_sent[(webis_df_multi_sent['original_sent_count']>=1)&(webis_df_multi_sent['paraphrase_sent_count']>=1)]\n","# webis_df_multi_sent.drop(columns=['original_sent_count','paraphrase_sent_count']).to_csv('webis_df_multi_sent.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oa9PzTr0JR9R","outputId":"e2914656-8999-4b36-8a58-a71080755529"},"outputs":[{"data":{"text/plain":["['even m  comte would spurn such irrational reasoning',\n"," 'however m  comte adheres himself to a fruitful belief one which he will offer us instead the scientific method',\n"," 'this scientific method has in fact just been observed']"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["# webis_df_multi_sent['paraphrase_text'].iloc[0]"]},{"cell_type":"markdown","metadata":{"id":"x1-rdNM9JR9R"},"source":["###### webis_df.to_csv(r'D:\\NLP\\webis_df.tsv',sep='\\t',index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}